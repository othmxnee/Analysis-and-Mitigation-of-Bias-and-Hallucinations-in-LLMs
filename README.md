# Analysis-and-Mitigation-of-Bias-and-Hallucinations-in-LLMs
This project investigates two critical issues in Large Language Models (LLMs): bias and hallucinations. The goal is to detect, analyze, and mitigate these problems using practical techniques
